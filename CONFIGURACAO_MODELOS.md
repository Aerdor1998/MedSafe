# Configura√ß√£o dos Modelos Ollama para MedSafe

## ü§ñ Modelos Utilizados

O MedSafe usa dois modelos espec√≠ficos do Ollama para diferentes funcionalidades:

### 1. **Qwen3:4b** - Modelo de Texto Principal
- **Fun√ß√£o**: An√°lise farmacol√≥gica, contraindica√ß√µes, intera√ß√µes
- **Tamanho**: ~2.3GB
- **Uso**: Processamento de texto, an√°lise de dados de pacientes, gera√ß√£o de recomenda√ß√µes

### 2. **Qwen2.5VL:7b** - Modelo de Vis√£o
- **Fun√ß√£o**: An√°lise de imagens de medicamentos
- **Tamanho**: ~4.1GB  
- **Uso**: OCR inteligente, identifica√ß√£o de medicamentos por imagem, extra√ß√£o de texto de embalagens

## üîß Configura√ß√£o Autom√°tica

### Instala√ß√£o R√°pida
```bash
# 1. Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Configurar modelos automaticamente
python setup_ollama.py

# 3. Iniciar sistema
./start.sh
```

### Configura√ß√£o Manual
```bash
# Iniciar Ollama
ollama serve

# Em outro terminal, baixar modelos
ollama pull qwen3:4b
ollama pull qwen2.5vl:7b

# Verificar instala√ß√£o
ollama list
```

## ‚öôÔ∏è Configura√ß√µes T√©cnicas

### Modelo de Texto (qwen3:4b)
```python
CONFIGURACAO_TEXTO = {
    "model": "qwen3:4b",
    "base_url": "http://localhost:11434/v1",
    "api_key": "ollama",
    "api_type": "openai",
    "temperature": 0.3,    # Baixa para precis√£o m√©dica
    "max_tokens": 2048,
    "top_p": 0.9
}
```

### Modelo de Vis√£o (qwen2.5vl:7b)
```python
CONFIGURACAO_VISAO = {
    "model": "qwen2.5vl:7b", 
    "base_url": "http://localhost:11434/v1",
    "api_key": "ollama",
    "api_type": "openai",
    "temperature": 0.1,    # Muito baixa para OCR preciso
    "max_tokens": 1500
}
```

## üéØ Funcionalidades por Modelo

### Qwen3:4b (An√°lise de Texto)
‚úÖ **An√°lise de contraindica√ß√µes**
- Processa dados do paciente
- Aplica diretrizes OMS/ANVISA
- Identifica fatores de risco

‚úÖ **An√°lise de intera√ß√µes medicamentosas**
- Compara com medicamentos atuais
- Classifica tipos de intera√ß√£o
- Avalia gravidade dos riscos

‚úÖ **Gera√ß√£o de recomenda√ß√µes**
- Cria orienta√ß√µes personalizadas
- Sugere monitoramento necess√°rio
- Prop√µe alternativas quando aplic√°vel

### Qwen2.5VL:7b (An√°lise de Imagens)
‚úÖ **Identifica√ß√£o de medicamentos**
- Reconhece nomes comerciais
- Identifica princ√≠pios ativos
- Extrai informa√ß√µes de dosagem

‚úÖ **OCR avan√ßado**
- L√™ texto em embalagens
- Processa bulas e r√≥tulos
- Identifica informa√ß√µes de lote/validade

‚úÖ **Valida√ß√£o de imagens**
- Confirma se √© medicamento
- Avalia qualidade da imagem
- Sugere melhorias na captura

## üîç Monitoramento e Logs

### Verifica√ß√£o de Status
```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Verificar modelos instalados
ollama list

# Status via API MedSafe
curl http://localhost:8000/api/health
```

### Logs de Performance
O sistema registra automaticamente:
- Tempo de resposta dos modelos
- Taxa de sucesso do OCR
- Confian√ßa das an√°lises
- Uso de recursos (CPU/RAM)

## üö® Troubleshooting

### Problemas Comuns

**Ollama n√£o inicia:**
```bash
# Verificar portas em uso
sudo netstat -tulpn | grep 11434

# Reiniciar servi√ßo
pkill ollama
ollama serve
```

**Modelo n√£o baixa:**
```bash
# Verificar espa√ßo em disco
df -h

# Tentar download manual
ollama pull qwen3:4b --verbose
```

**IA de vis√£o falha:**
```bash
# Testar modelo diretamente
ollama run qwen2.5vl:7b "Analise esta imagem"

# Verificar logs do sistema
tail -f logs/error_*.jsonl
```

### Fallbacks Autom√°ticos

O sistema tem fallbacks integrados:

1. **Se qwen2.5vl:7b n√£o dispon√≠vel**: Usa Tesseract OCR tradicional
2. **Se qwen3:4b n√£o dispon√≠vel**: Usa an√°lise baseada apenas no banco de dados
3. **Se Ollama offline**: Sistema funciona com funcionalidade limitada

## üìä Requisitos de Sistema

### M√≠nimos
- **RAM**: 8GB (4GB para modelo + 4GB sistema)
- **Storage**: 10GB livres
- **CPU**: 4 cores (recomendado)
- **GPU**: Opcional (acelera processamento)

### Recomendados
- **RAM**: 16GB (melhor performance)
- **Storage**: 20GB livres (para logs e cache)
- **CPU**: 8 cores ou mais
- **GPU**: NVIDIA com CUDA (opcional)

## üîê Seguran√ßa e Privacidade

### Processamento Local
‚úÖ **Todos os dados permanecem no seu computador**
‚úÖ **Nenhuma informa√ß√£o √© enviada para servidores externos**
‚úÖ **Modelos rodam completamente offline**
‚úÖ **Conformidade total com LGPD**

### Dados Sens√≠veis
- Informa√ß√µes de pacientes s√£o processadas apenas localmente
- Imagens de medicamentos n√£o saem do dispositivo
- An√°lises s√£o feitas sem conex√£o com a internet
- Logs podem ser anonimizados automaticamente

## üìà Otimiza√ß√£o de Performance

### Para melhor velocidade:
```bash
# Usar GPU se dispon√≠vel
OLLAMA_GPU=1 ollama serve

# Aumentar contexto se necess√°rio
OLLAMA_NUM_PARALLEL=2 ollama serve
```

### Para economizar RAM:
```bash
# Limitar uso de mem√≥ria
OLLAMA_MODELS_PATH=/path/to/ssd ollama serve
```

### Configura√ß√µes avan√ßadas:
```env
# No arquivo config.env
OLLAMA_KEEP_ALIVE=5m
OLLAMA_MAX_LOADED_MODELS=2
OLLAMA_FLASH_ATTENTION=1
```

## üìû Suporte

Para problemas espec√≠ficos com os modelos:

1. **Verificar logs**: `logs/ai_service_*.jsonl`
2. **Testar modelos**: `python setup_ollama.py`
3. **Verificar recursos**: Monitor de sistema
4. **Documenta√ß√£o Ollama**: https://ollama.com/docs

---

**Nota**: Os modelos Qwen s√£o especialmente adequados para an√°lise farmacol√≥gica devido √† sua precis√£o em contextos m√©dicos e capacidade multil√≠ngue (portugu√™s/ingl√™s).